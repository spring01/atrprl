*** finished ***
make atrp_env.py a base class
different action modes can be derived from base
different reward modes can be derived from base
adding never exceeds cap
implement a learning curve drawing script
compare cat2_0721_obs_stable with cat_0720 to get whether it can train with observing only stable species: seems ok
compare cat3_0721_net_200_100 with cat_0720 to get whether it can train with simpler net: seems 200 100 is better than 400 200 100
compare cat4_0721_gamma0.99 with cat_0720 to get whether it can train with gamma = 0.99: seems 0.99 is better than 0.999
*compare cat4_0722_2frame_2step with cat3_0722_ref02 to see if 2-frame-2-step works: 2-frame-2-step fails
compare cat1_0721_obs_single_frame with cat_0720 to get whether it can train with single frame input
   note: currently act_steps is also 1, may cause issues
*compare cat1_0721_obs_single_frame with cat2_0722_obs_1frame_4step to rule out the effect of act_steps: 1frame_4step sorta works sorta not
effect of rollout_maxlen: 20 might be best
effect of learning rate: 1e-4 might be best
single frame input (with 4 action repeats) is probably sufficient


*** summaries ***
current ref: --net_arch 200 100, gamma = 0.99, lr = 1e-4, rollout_maxlen 20, num_frames 1, act_steps 4


*** running ***





*** todo ***
run random simulations to get some interesting looking distributions
implement A3C+NoisyNet
implement NEC

